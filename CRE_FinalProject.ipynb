{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CRE Research Project for Sentiment Analysis Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the dataset\n",
    "import pandas\n",
    "import pickle\n",
    "with open(\"SentAnalysisData.csv\", 'rb') as f:\n",
    "    data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11327, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get original size/shape of dataset. \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>There are tons of other paintings that I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Yet the dog had grown old and less capable , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I get into the tube or the train without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>This last may be a source of considerable disq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger</td>\n",
       "      <td>She disliked the intimacy he showed towards so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>sadness</td>\n",
       "      <td>My sweetheart left me, or rather we decided to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Well , it's too bad that we like different kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>neutral</td>\n",
       "      <td>It sure is .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>sadness</td>\n",
       "      <td>He ’ s got laid off again . I do feel sorry fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>anger</td>\n",
       "      <td>When stupid people push me during rush time in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11327 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text\n",
       "0     neutral   There are tons of other paintings that I thin...\n",
       "1     sadness  Yet the dog had grown old and less capable , a...\n",
       "2        fear  When I get into the tube or the train without ...\n",
       "3        fear  This last may be a source of considerable disq...\n",
       "4       anger  She disliked the intimacy he showed towards so...\n",
       "...       ...                                                ...\n",
       "3388  sadness  My sweetheart left me, or rather we decided to...\n",
       "3389  sadness  Well , it's too bad that we like different kin...\n",
       "3390  neutral                                      It sure is . \n",
       "3391  sadness  He ’ s got laid off again . I do feel sorry fo...\n",
       "3392    anger  When stupid people push me during rush time in...\n",
       "\n",
       "[11327 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy        2326\n",
      "sadness    2317\n",
      "anger      2259\n",
      "neutral    2254\n",
      "fear       2171\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#distribution of each of the class labels. 5 Emotions to train for. \n",
    "print(data.Emotion.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up the data for training, validation, and test\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test sets using data 80/20 split on train and test\n",
    "       #leave alone minus preprocesssing\n",
    "         #----#          #-----#\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.Text, data.Emotion, test_size = 0.20, random_state = 77)\n",
    "#txt,    txt,   emotion, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#working with a pd series\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "#Preprocessing of the data\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes numbers \n",
    "###X_train = X_train.str.replace('\\d+', '')\n",
    "###X_test = X_test.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Punctuation removal\n",
    "#X_train = X_train.str.replace('[^\\w\\s]','')\n",
    "#X_test = X_test.str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing method #1\n",
    "#Contraction reomoval: https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/\n",
    "import contractions\n",
    "def remove_contractions_text(text):\n",
    "    return [contractions.fix(w) for w in word_tokenizer.tokenize(text)]\n",
    "\n",
    "X_train = X_train.apply(remove_contractions_text)\n",
    "X_test = X_test.apply(remove_contractions_text)\n",
    "\n",
    "X_train=X_train.str.join(\" \")\n",
    "X_test=X_test.str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont use bad lemmatization function\n",
    "#lemmatazation: https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/\n",
    "#lemmatazation: https://stackoverflow.com/questions/47557563/lemmatization-of-all-pandas-cells\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_tokenizer.tokenize(text)]\n",
    "\n",
    "#X_train = X_train.str.replace('[^\\w\\s]','')\n",
    "X_train = X_train.apply(lemmatize_text)\n",
    "X_test = X_test.apply(lemmatize_text)\n",
    "\n",
    "X_train=X_train.str.join(\" \")\n",
    "X_test=X_test.str.join(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing method #2\n",
    "#Snowball stemmer to stem the words\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_text(text):\n",
    "    return [stemmer.stem(w) for w in word_tokenizer.tokenize(text)]\n",
    "\n",
    "X_train = X_train.apply(stem_text)\n",
    "X_test = X_test.apply(stem_text)\n",
    "\n",
    "X_train=X_train.str.join(\" \")\n",
    "X_test=X_test.str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing method #3\n",
    "#Remove stopwords with the help of the nltk library stopwords. Import words to compare against and remove.\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "X_train = X_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "X_test= X_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing method #4\n",
    "#Removal of certain characters/symbols from the dataset. I skimmed over the data and rmoved some reoccuring symbols\n",
    "#that were littered about in the dataset.\n",
    "X_train = X_train.map(lambda x: x.strip('’#$%@'))\n",
    "X_test = X_test.map(lambda x: x.strip('’#$%@'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing method #5\n",
    "#Convert to lowercase. \n",
    "X_train = X_train.str.lower()\n",
    "X_test = X_test.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     ! also finish write sale report bos . end , f...\n",
       "6    find chosen collect norm chines aphasia (i con...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validate that preprocessing methods worked as anticipated\n",
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     ! also finish write sale report bos . end , f...\n",
       "6    find chosen collect norm chines aphasia (i con...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validate that preprocessing methods worked as anticipated\n",
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9061,)\n",
      "(2266,)\n"
     ]
    }
   ],
   "source": [
    "#Validate that preprocessing methods did not change the size of the data\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#From this point leave the test set alone until the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation using train 60/20 split on train and validation\n",
    "#This is for development and is often called the development set. \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 45)\n",
    "#txt, txt, emotion, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6795,)\n",
      "(2266,)\n"
     ]
    }
   ],
   "source": [
    "#Validate that the size of the data was changed accordingly\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution of test set\n",
      "joy        460\n",
      "anger      458\n",
      "neutral    451\n",
      "sadness    450\n",
      "fear       447\n",
      "Name: Emotion, dtype: int64\n",
      "class distribution of train set\n",
      "joy        1396\n",
      "sadness    1391\n",
      "anger      1363\n",
      "neutral    1352\n",
      "fear       1293\n",
      "Name: Emotion, dtype: int64\n",
      "class distribution of validation set\n",
      "sadness    476\n",
      "joy        470\n",
      "neutral    451\n",
      "anger      438\n",
      "fear       431\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#use the validation set that was just created. \n",
    "print(\"class distribution of test set\")\n",
    "print(y_test.value_counts())  #distribution of test set\n",
    "\n",
    "print(\"class distribution of train set\")\n",
    "print(y_train.value_counts())  #distribution of train set\n",
    "\n",
    "print(\"class distribution of validation set\")\n",
    "print(y_val.value_counts())  #distribution of val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use TF-IDF: Numerical representaion, number of features increased from 300 to 1500. \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 1500, stop_words='english')\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birthday': 158, 'veri': 1431, 'close': 257, 'friend': 559, 'sent': 1179, 'huge': 680, 'pleas': 996, 'react': 1072, 'bad': 118, 'start': 1276, 'season': 1165, 'quarrel': 1059, 'younger': 1496, 'sister': 1216, 'becaus': 134, 'said': 1148, 'someth': 1240, 'boyfriend': 182, 'consequ': 290, 'look': 803, 'saw': 1153, 'thought': 1367, 'wa': 1438, 'tri': 1400, 'separ': 1180, 'misunderstood': 874, 'girl': 583, 'love': 811, 'accept': 18, 'propos': 1045, 'everyth': 461, 'went': 1460, 'want': 1443, 'previous': 1025, 'feel': 517, 'lower': 814, 'class': 249, 'dure': 420, 'summer': 1320, 'join': 725, 'research': 1110, 'group': 614, 'chines': 243, 'univers': 1421, 'student': 1303, 'knew': 742, 'lot': 809, 'year': 1490, 'later': 756, 'travel': 1395, 'china': 242, 'got': 598, 'die': 381, 'week': 1456, 'come': 271, 'togeth': 1380, 'funer': 566, 'bodi': 170, 'time': 1376, 'felt': 521, 'extrem': 490, 'sad': 1145, 'yes': 1492, 'like': 786, 'coffe': 264, 'bring': 188, 'famili': 504, 'explain': 487, 'make': 826, 'sens': 1178, 'shower': 1200, 'dress': 409, 'teeth': 1346, 'door': 404, 'half': 622, 'hour': 675, 'search': 1164, 'hous': 677, 'fall': 502, 'final': 528, 'offer': 926, 'place': 991, 'refus': 1091, 'relat': 1096, 'parent': 959, 'pay': 974, 'turn': 1409, 'upset': 1425, 'dad': 330, 'threaten': 1369, 'cut': 327, 'grief': 611, 'stricken': 1296, 'mr': 889, 'yesterday': 1493, 'begin': 141, 'new': 906, 'cours': 314, 'afraid': 37, 'encount': 443, 'problem': 1032, 'fail': 496, 'subject': 1309, 'future': 571, 'car': 211, 'heard': 641, 'drink': 411, 'beer': 138, 'thank': 1357, 'danc': 334, 'sorri': 1247, 'sale': 1149, 'onli': 937, 'store': 1288, 'mayb': 846, 'delight': 361, 'took': 1386, 'discuss': 392, 'work': 1478, 'listen': 792, 'utter': 1428, 'profession': 1036, 'secret': 1169, 'impress': 695, 'ran': 1068, 'race': 1064, 'cal': 203, 'track': 1391, 'team': 1342, 'terribl': 1353, 'coach': 262, 'sit': 1217, 'realli': 1078, 'abl': 14, 'run': 1143, 'alway': 54, 'meet': 856, 'tomorrow': 1383, 'hurri': 683, 'right': 1123, 'school': 1159, 'shout': 1199, 'argument': 88, 'abil': 13, 'carri': 214, 'particular': 964, 'duti': 421, 'correct': 305, 'way': 1451, 'arriv': 93, 'university': 1422, 'know': 745, 'yeah': 1489, 'noth': 917, 'clear': 253, 'test': 1356, 'spend': 1260, 'holiday': 660, 'night': 910, 'nois': 912, 'scare': 1155, 'house': 678, 'moment': 877, 'unknown': 1424, 'pick': 986, 'person': 982, 'think': 1364, 'scienc': 1160, 'girlfriend': 584, 'gave': 576, 'left': 771, 'minut': 870, 'video': 1433, 'date': 339, 'onlin': 938, 'provid': 1049, 'servic': 1181, 'face': 493, 'guy': 619, 'talk': 1334, 'beaten': 131, 'wed': 1455, 'party': 966, 'suffer': 1315, 'illness': 690, 'quit': 1063, 'hear': 640, 'roommat': 1134, 'truth': 1407, 'deepli': 354, 'hurt': 684, 'learn': 764, 'good': 596, 'man': 829, 'express': 488, 'point': 999, 'tear': 1343, 'result': 1117, 'exam': 466, 'failed': 497, 'readi': 1074, 'tell': 1349, 'mark': 834, 'fool': 541, 'dare': 336, 'marri': 837, 'bet': 149, 'dollar': 403, 'let': 775, 'pass': 968, 'entranc': 452, 'finish': 530, 'difficult': 384, 'period': 981, 'life': 781, 'desir': 372, 'death': 348, 'grandfath': 604, 'kind': 736, 'stupid': 1307, 'footbal': 543, 'fear': 515, 'happen': 627, 'stay': 1280, 'bus': 196, 'onc': 936, 'long': 801, 'cancer': 209, 'drive': 412, 'nervous': 905, 'kept': 731, 'fix': 532, 'smile': 1231, 'promis': 1042, 'sorrow': 1248, 'caus': 220, 'german': 580, 'save': 1152, 'day': 343, 'shot': 1197, 'annoy': 68, 'higher': 655, 'ask': 97, 'whi': 1463, 'job': 723, 'tire': 1378, 'treatment': 1397, 'depress': 368, 'patient': 972, 'given': 586, 'movi': 888, 'sound': 1251, 'easi': 426, 'wrong': 1486, 'number': 919, 'befor': 139, 'sure': 1326, 'polit': 1002, 'knowledg': 746, 'view': 1434, 'mother': 885, 'angri': 61, 'eat': 428, 'lose': 806, 'worri': 1481, 'experienc': 485, 'opposit': 944, 'direct': 386, 'small': 1228, 'inter': 713, 'sport': 1265, 'event': 457, 'favourit': 514, 'nation': 896, 'player': 995, 'joy': 728, 'world': 1480, 'cup': 324, 'match': 841, 'deep': 353, 'disquiet': 395, 'use': 1426, 'food': 540, 'spirit': 1263, 'disappoint': 389, 'news': 907, 'sick': 1203, 'relative': 1098, 'heartbroken': 644, 'return': 1119, 'remain': 1103, 'rest': 1115, 'natur': 897, 'line': 789, 'cousin': 316, 'unfortun': 1419, 'away': 115, 'sudden': 1314, 'dream': 407, 'dead': 345, 'okay': 933, 'anoth': 69, 'soon': 1246, 'men': 858, 'month': 880, 'short': 1196, 'home': 662, 'far': 508, 'times': 1377, 'came': 205, 'wait': 1439, 'master': 839, 'li': 778, 'luck': 816, 'kick': 733, 'corner': 304, 'amus': 58, 'oh': 930, 'hi': 652, 'teas': 1344, 'score': 1162, 'exams': 472, 'law': 758, 'consid': 291, 'human': 681, 'prevent': 1024, 'offend': 925, 'parti': 962, 'child': 239, 'near': 898, 'knock': 744, 'doe': 400, 'practic': 1011, 'therefor': 1360, 'alon': 52, 'weekend': 1457, 'busi': 198, 'sever': 1185, 'things': 1363, 'snake': 1233, 'bank': 123, 'custom': 326, 'slow': 1226, 'shift': 1191, 'mif': 866, 'recent': 1083, 'cool': 302, 'buy': 199, 'magazin': 823, 'invit': 716, 'gift': 582, 'happi': 629, 'mani': 831, 'healthi': 639, 'glad': 587, 'organ': 948, 'chemistri': 236, 'examination': 468, 'visit': 1436, 'strang': 1291, 'commit': 273, 'arm': 89, 'assault': 98, 'receiv': 1082, 'notic': 918, 'similar': 1208, 'film': 527, 'someon': 1239, 'stole': 1283, 'sink': 1214, 'cheer': 235, 'punish': 1055, 'littl': 793, 'opinion': 942, 'middl': 863, 'dreamt': 408, 'stranger': 1292, 'appoint': 82, 'hope': 666, 'stori': 1289, 'hold': 659, 'fuck': 564, 'everytim': 462, 'rt': 1139, 'lucki': 817, 'secur': 1170, 'officialpwg': 929, 'age': 40, 'heat': 645, 'live': 794, 'room': 1133, 'cook': 301, 'cloth': 260, 'caught': 219, 'cold': 265, 'shop': 1195, 'round': 1137, 'big': 152, 'build': 194, 'end': 445, 'road': 1128, 'told': 1381, 'anymor': 73, 'exchang': 475, 'ball': 120, 'lost': 808, 'standard': 1273, 'roll': 1131, 'ground': 613, 'fellow': 520, 'passeng': 970, 'ignor': 688, 'actual': 27, 'flat': 534, 'plan': 992, 'paint': 957, 'brother': 192, 'graduat': 602, 'christma': 245, 'eve': 455, 'parents': 960, 'recal': 1081, 'past': 971, 'ah': 44, 'deject': 358, 'ad': 28, 'john': 724, 'chanc': 228, 'trust': 1406, 'confid': 286, 'blame': 163, 'doctor': 398, 'town': 1390, 'son': 1244, 'examin': 467, 'medic': 852, 'institut': 708, 'mari': 833, 'horrifi': 668, 'howev': 679, 'stop': 1287, 'becam': 133, 'father': 511, 'instead': 707, 'treat': 1396, 'borrow': 175, 'destroy': 378, 'ticket': 1373, 'exact': 465, 'unexpect': 1417, 'imposs': 694, 'autumn': 110, 'nice': 909, 'washington': 1446, 'soft': 1236, 'heart': 642, 'excel': 474, 'simpli': 1210, 'bear': 129, 'behavior': 142, 'concern': 282, 'welcom': 1459, 'vacat': 1429, 'madam': 822, 'aliv': 50, 'offic': 927, 'refer': 1088, 'miser': 871, 'rough': 1136, 'area': 86, 'lock': 797, 'physic': 985, 'midterm': 865, 'matter': 845, 'awar': 114, 'ani': 64, 'gentleman': 579, 'describ': 369, 'sinc': 1211, 'broke': 190, 'set': 1182, 'pictur': 987, 'peopl': 976, 'war': 1444, 'rob': 1129, 'walk': 1441, 'midnight': 864, 'reach': 1071, 'thousand': 1368, 'ear': 422, 'closest': 259, 'money': 879, 'bought': 179, 'record': 1085, 'stolen': 1284, 'threw': 1370, 'surpris': 1327, 'ill': 689, 'spare': 1255, 'grandmoth': 605, 'died': 382, 'select': 1172, 'malawi': 827, 'excit': 476, 'member': 857, 'qualifi': 1058, 'level': 777, 'decid': 350, 'fell': 519, 'window': 1468, 'realiz': 1077, 'admit': 32, 'high': 654, 'realis': 1076, 'gone': 595, 'alarm': 48, 'late': 755, 'bed': 136, 'read': 1073, 'leg': 772, 'hand': 624, 'connect': 289, 'proceed': 1034, 'case': 215, 'abus': 17, 'control': 297, 'temper': 1350, 'rush': 1144, 'book': 172, 'report': 1108, 'teacher': 1341, 'forgiv': 548, 'word': 1477, 'neighbour': 904, 'thief': 1361, 'seven': 1184, 'dinner': 385, 'becom': 135, 'sign': 1205, 'bos': 176, 'park': 961, 'young': 1495, 'woman': 1474, 'stare': 1275, 'frighten': 562, 'suck': 1313, 'grab': 600, 'frightened': 563, 'angry': 62, 'ride': 1122, 'speed': 1259, 'km': 740, 'snow': 1234, 'reason': 1079, 'met': 862, 'present': 1018, 'boat': 168, 'trip': 1402, 'england': 446, 'sweden': 1329, 'till': 1375, 'drunken': 419, 'repli': 1107, 'exasper': 473, 'jump': 729, 'miss': 872, 'ok': 932, 'boy': 181, 'paul': 973, 'certain': 223, 'push': 1057, 'seat': 1166, 'appar': 78, 'complet': 279, 'abroad': 15, 'taken': 1333, 'agre': 43, 'tenni': 1351, 'kid': 734, 'cycl': 328, 'court': 315, 'desol': 374, 'hey': 651, 'beach': 128, 'hit': 658, 'restaur': 1116, 'cafe': 201, 'damn': 332, 'enter': 450, 'secondari': 1168, 'critic': 321, 'despond': 377, 'involv': 717, 'traffic': 1392, 'accident': 20, 'studi': 1304, 'field': 523, 'loud': 810, 'born': 174, 'mention': 859, 'thing': 1362, 'air': 46, 'stomach': 1285, 'idea': 687, 'stair': 1271, 'anxious': 72, 'demand': 362, 'pleasur': 998, 'beat': 130, 'watch': 1448, 'ahead': 45, 'everi': 458, 'low': 813, 'fight': 525, 'pressur': 1021, 'family': 506, 'effect': 432, 'emot': 441, 'failur': 498, 'open': 939, 'attack': 102, 'friday': 558, 'leav': 766, 'sir': 1215, '10': 1, 'morn': 883, 'somewher': 1243, 'market': 835, 'chairman': 226, 'wood': 1476, 'seen': 1171, 'address': 29, 'telephon': 1347, 'prospect': 1046, 'horror': 669, 'stress': 1295, 'pretti': 1023, 'usual': 1427, 'brand': 183, 'sleep': 1221, 'eyes': 492, 'scene': 1157, 'senior': 1177, 'junior': 730, 'grew': 610, 'progress': 1040, 'perfect': 978, 'bar': 124, 'price': 1026, 'speech': 1258, 'whatev': 1462, 'import': 693, 'hang': 626, 'longer': 802, 'better': 150, 'alreadi': 53, 'success': 1312, 'succeed': 1311, 'lugubri': 818, 'answer': 70, 'share': 1189, 'york': 1494, 'second': 1167, 'plane': 993, 'aunt': 108, 'touch': 1388, 'funeral': 567, 'believ': 144, 'guess': 616, 'tonight': 1385, 'exhilar': 479, 'attent': 105, 'extra': 489, 'beauti': 132, 'relationship': 1097, 'manner': 832, 'singl': 1213, 'cost': 306, 'experi': 484, 'dog': 401, 'public': 1053, 'anim': 65, 'avoid': 112, 'delay': 359, 'write': 1485, 'certif': 224, 'elder': 435, 'evid': 463, 'earli': 423, 'centuri': 222, 'measur': 851, 'bang': 122, 'wonder': 1475, 'kitchen': 739, 'glanc': 588, 'phone': 984, 'play': 994, 'homework': 663, 'sometim': 1241, 'hospital': 672, 'darl': 338, 'suppos': 1325, 'nobodi': 911, 'break': 184, 'thorough': 1366, 'mom': 876, 'anger': 60, 'wish': 1471, 'swept': 1330, 'bob': 169, 'head': 635, 'saturday': 1151, 'repeat': 1106, 'pas': 967, 'old': 934, 'librari': 779, 'applic': 81, 'programm': 1039, 'list': 791, 'board': 167, 'includ': 698, 'classmat': 251, 'concentr': 281, 'studies': 1305, 'troubl': 1403, 'lie': 780, 'clean': 252, 'tv': 1410, 'wow': 1484, 'richard': 1121, 'eye': 491, 'babi': 117, 'accord': 21, 'enrag': 449, 'charg': 230, 'situat': 1218, 'hopeless': 667, 'lone': 799, 'mean': 848, 'fun': 565, 'exercis': 478, 'countri': 311, 'eaten': 429, 'perform': 979, 'great': 608, 'swim': 1331, 'true': 1405, 'general': 578, 'oper': 940, 'thrill': 1371, 'fashion': 509, 'shoe': 1193, 'forward': 552, 'enjoy': 448, 'chat': 232, 'anguish': 63, 'tone': 1384, 'step': 1281, 'fault': 512, 'dark': 337, 'figur': 526, 'form': 550, 'forget': 547, 'bike': 154, 'health': 638, 'necessari': 900, 'known': 747, 'street': 1294, 'sell': 1174, 'twenti': 1411, 'camera': 206, 'nearbi': 899, 'mood': 882, 'lunch': 819, 'prejud': 1016, 'uncl': 1414, 'total': 1387, 'follow': 539, 'worst': 1483, 'god': 594, 'letter': 776, 'friends': 560, 'arrang': 91, 'insult': 709, 'hors': 670, 'hard': 631, 'chang': 229, 'wors': 1482, 'spread': 1267, 'dismay': 394, 'south': 1253, 'say': 1154, 'hospit': 671, 'accid': 19, 'piti': 990, 'overcom': 951, 'bit': 159, 'lip': 790, 'rememb': 1104, 'exampl': 470, 'colleg': 267, 'newspap': 908, 'cri': 320, 'everybodi': 459, 'particip': 963, 'older': 935, 'amaz': 55, '30': 10, 'yr': 1498, 'today': 1379, 'excus': 477, 'familiar': 505, 'privat': 1029, 'tabl': 1332, 'sourc': 1252, 'resent': 1111, 'help': 649, 'fact': 494, 'suggest': 1316, 'white': 1464, 'allow': 51, 'refund': 1090, 'check': 234, 'paper': 958, 'mistak': 873, 'understand': 1415, 'chief': 238, 'overjoy': 952, 'sun': 1321, 'harri': 632, 'role': 1130, 'mat': 840, 'support': 1324, 'livid': 795, 'polic': 1000, 'furious': 569, 'wit': 1472, 'blood': 165, 'problems': 1033, 'silli': 1207, 'obvious': 921, 'care': 213, 'fli': 535, 'understood': 1416, 'anyth': 75, 'lecture': 768, 'discov': 391, 'oil': 931, 'wall': 1442, 'results': 1118, 'meant': 849, 'major': 825, 'lesson': 774, 'semest': 1175, 'ice': 686, 'expect': 481, 'admiss': 31, 'ha': 620, 'larg': 754, 'land': 752, 'effort': 433, 'cancel': 208, 'fan': 507, 'argu': 87, 'song': 1245, 'aw': 113, 'mind': 868, 'suicide': 1317, 'unabl': 1413, 'calm': 204, 'sadden': 1146, 'attempt': 103, 'term': 1352, 'ex': 464, 'recov': 1086, 'lack': 749, 'lead': 761, 'lift': 783, 'damag': 331, '16': 5, 'cross': 322, 'hill': 656, 'rain': 1066, 'differ': 383, 'west': 1461, 'imagin': 691, 'stand': 1272, 'stood': 1286, 'sexual': 1186, 'math': 844, 'chase': 231, 'monday': 878, 'indign': 701, 'presid': 1019, 'hello': 648, 'partner': 965, '00': 0, 'approach': 84, 'laugh': 757, 'current': 325, 'affair': 35, 'music': 892, 'fine': 529, 'art': 94, 'elat': 434, 'futur': 570, 'attitud': 106, 'professor': 1037, 'fed': 516, 'xa': 1488, 'anyon': 74, 'hundr': 682, 'agit': 41, 'ring': 1124, 'conversation': 300, 'drove': 415, 'real': 1075, 'country': 312, 'forgot': 549, 'behaviour': 143, 'game': 573, 'drug': 417, 'woke': 1473, 'cover': 317, 'apolog': 77, 'voic': 1437, 'gleeful': 591, 'bicycl': 151, 'need': 901, 'grin': 612, 'rais': 1067, 'shoulder': 1198, 'heartbreak': 643, 'loss': 807, 'pet': 983, 'electr': 437, 'despit': 376, 'paid': 955, 'earlier': 424, 'bye': 200, 'messag': 861, 'slope': 1225, 'drew': 410, 'distanc': 396, 'tree': 1398, 'surviv': 1328, 'operation': 941, 'journey': 727, 'storm': 1290, 'strong': 1300, 'stir': 1282, 'wind': 1467, 'adult': 33, 'people': 977, 'leader': 762, 'lean': 763, 'develop': 380, 'weak': 1452, 'poor': 1003, 'earth': 425, 'drop': 414, 'incid': 697, 'pound': 1008, 'pub': 1052, 'garden': 574, 'star': 1274, 'sing': 1212, 'joke': 726, 'order': 947, 'depart': 365, 'learnt': 765, 'goodby': 597, 'articl': 95, 'dirti': 387, 'program': 1038, 'american': 57, 'militari': 867, 'cope': 303, 'pour': 1009, 'comput': 280, 'favorit': 513, 'respons': 1114, 'knife': 743, 'pain': 956, 'english': 447, 'infuri': 703, 'risk': 1126, 'lectur': 767, 'note': 916, 'station': 1279, 'train': 1393, 'drunk': 418, 'purpos': 1056, 'post': 1007, 'silent': 1206, 'lay': 759, 'dawn': 342, 'london': 798, 'announc': 67, 'languag': 753, 'continu': 296, '12': 3, 'divorc': 397, 'question': 1060, 'david': 341, 'reliev': 1102, 'slept': 1222, 'best': 148, 'tom': 1382, '15': 4, 'reveal': 1120, 'piec': 988, 'colour': 270, 'chicken': 237, 'breath': 185, 'local': 796, 'inconsol': 699, 'apart': 76, 'mrs': 890, 'club': 261, 'somewhat': 1242, 'increas': 700, 'pull': 1054, 'inform': 702, 'manag': 830, 'atmospher': 101, 'goal': 593, 'sat': 1150, 'content': 294, 'cat': 217, 'male': 828, 'normal': 915, 'classic': 250, 'send': 1176, 'bedroom': 137, 'championship': 227, 'noon': 914, 'coupl': 313, 'insid': 705, 'shake': 1187, 'cheat': 233, 'proper': 1044, 'water': 1449, 'psycholog': 1051, 'releas': 1100, 'gaze': 577, 'deal': 346, 'document': 399, 'prepar': 1017, 'quick': 1061, 'valu': 1430, 'histori': 657, 'expected': 482, 'deliber': 360, 'provok': 1050, 'da': 329, 'educ': 431, 'grade': 601, 'bother': 177, 'consider': 292, 'defend': 355, 'freak': 553, 'perhap': 980, 'driver': 413, 'slight': 1223, 'irrit': 718, 'luci': 815, 'social': 1235, 'popular': 1004, 'peev': 975, 'stuck': 1302, 'coat': 263, 'black': 162, 'shoot': 1194, 'convers': 299, 'settl': 1183, 'issu': 719, 'mess': 860, 'hair': 621, 'non': 913, 'honest': 664, 'improv': 696, 'broken': 191, 'speak': 1256, 'slowli': 1227, 'pretend': 1022, 'els': 438, 'somebodi': 1238, 'river': 1127, 'card': 212, 'pregnant': 1015, 'shall': 1188, 'citi': 248, 'fresh': 556, 'chair': 225, 'airport': 47, 'thirti': 1365, 'ghost': 581, 'limit': 788, 'hot': 674, 'trick': 1401, 'taylor': 1339, 'disconsol': 390, 'win': 1466, 'hate': 633, 'medium': 855, 'captain': 210, 'machin': 820, 'declar': 352, 'materi': 843, 'themselv': 1358, 'prefer': 1014, 'mad': 821, 'scared': 1156, 'daughter': 340, 'began': 140, 'embarrass': 439, 'factor': 495, 'fish': 531, 'lake': 751, 'strict': 1297, 'friendship': 561, 'haunt': 634, 'dentist': 364, 'birth': 157, 'wife': 1465, 'roof': 1132, 'appreci': 83, 'wrote': 1487, 'radio': 1065, 'danger': 335, 'months': 881, 'smell': 1230, '1983': 6, '1984': 7, 'outsid': 950, 'kiss': 738, 'spent': 1261, 'object': 920, 'tour': 1389, 'crowd': 323, 'foot': 542, 'design': 371, 'colleagu': 266, 'legal': 773, 'tie': 1374, 'probabl': 1031, 'box': 180, 'brick': 186, 'afterward': 39, 'count': 309, 'bore': 173, 'crazi': 319, 'jim': 722, 'unfair': 1418, 'unjust': 1423, 'chosen': 244, 'forc': 544, 'happened': 628, 'wear': 1453, 'climb': 254, 'bull': 195, 'posit': 1005, 'bush': 197, 'block': 164, 'fast': 510, 'kill': 735, 'foreign': 545, 'camp': 207, 'red': 1087, 'holidays': 661, 'compani': 276, 'days': 344, 'overwhelm': 953, 'british': 189, 'armi': 90, 'belong': 145, 'held': 647, 'wave': 1450, 'cake': 202, 'stuff': 1306, 'desk': 373, 'recently': 1084, 'children': 241, 'scold': 1161, 'bright': 187, 'devast': 379, 'wine': 1469, 'employ': 442, 'immedi': 692, 'initi': 704, 'regular': 1095, 'regret': 1094, 'odd': 924, 'ladi': 750, 'lord': 805, 'jealous': 721, 'transport': 1394, 'pink': 989, 'communic': 275, 'decis': 351, 'regard': 1092, 'bitter': 161, 'account': 22, 'intent': 712, 'pack': 954, 'august': 107, 'primari': 1027, 'stream': 1293, 'respect': 1113, 'style': 1308, 'remind': 1105, '20': 9, 'predict': 1012, 'hong': 665, 'kong': 748, 'author': 109, 'shock': 1192, 'basic': 127, '1st': 8, 'ago': 42, 'bird': 156, 'downcast': 406, 'floor': 537, 'elect': 436, 'years': 1491, 'everyon': 460, 'comment': 272, 'investig': 715, 'appli': 80, 'approv': 85, 'advic': 34, 'medicin': 853, 'college': 268, 'asian': 96, 'marriag': 838, 'funni': 568, 'oral': 945, 'absolut': 16, 'row': 1138, 'opportun': 943, 'throw': 1372, 'possibl': 1006, 'failure': 499, 'closer': 258, 'forest': 546, 'self': 1173, 'wast': 1447, 'tuesday': 1408, 'fortun': 551, 'flare': 533, 'green': 609, 'appear': 79, 'le': 760, 'hide': 653, 'accus': 23, 'terrifi': 1354, 'heavi': 646, 'rule': 1141, 'achiev': 24, 'interview': 714, 'bite': 160, 'special': 1257, 'main': 824, 'worker': 1479, 'youngster': 1497, 'winter': 1470, 'flower': 538, 'crash': 318, 'dogs': 402, 'sum': 1319, 'tall': 1335, 'lili': 787, 'simpl': 1209, 'hours': 676, 'morning': 884, 'passed': 969, 'grandmother': 606, 'cinema': 246, 'blow': 166, 'concert': 283, 'clinic': 255, 'shown': 1201, 'weather': 1454, 'barbecu': 125, 'free': 554, 'biggest': 153, 'loos': 804, 'sunday': 1322, 'husband': 685, 'mce': 847, 'examinations': 469, 'america': 56, 'emerg': 440, 'fair': 500, 'bag': 119, 'spot': 1266, 'strike': 1298, 'act': 25, 'knee': 741, 'press': 1020, 'polici': 1001, 'depend': 367, 'govern': 599, 'offici': 928, 'power': 1010, 'superior': 1323, 'stage': 1270, 'gain': 572, 'childhood': 240, 'especi': 453, 'weeks': 1458, 'meanwhil': 850, 'teach': 1340, 'staff': 1269, 'prefect': 1013, 'king': 737, 'reflect': 1089, 'slip': 1224, 'afternoon': 38, 'encourag': 444, 'stubborn': 1301, 'ourselv': 949, 'unit': 1420, 'belov': 146, 'grand': 603, 'orchestra': 946, 'handl': 625, 'prize': 1030, 'expens': 483, 'dear': 347, 'girls': 585, 'fiance': 522, 'nasti': 895, 'state': 1277, 'besid': 147, 'jack': 720, 'ecstat': 430, 'type': 1412, 'minist': 869, 'mum': 891, 'lik': 785, 'movement': 887, '50': 12, 'condit': 284, 'spring': 1268, 'regist': 1093, 'arrest': 92, 'marks': 836, 'experience': 486, 'competition': 278, 'example': 471, 'scholarship': 1158, 'sea': 1163, 'gun': 618, 'degre': 357, 'modigliani': 875, 'tax': 1337, 'congratul': 288, 'entir': 451, 'stroke': 1299, 'grant': 607, 'french': 555, 'attend': 104, 'bark': 126, 'easili': 427, 'shut': 1202, 'flood': 536, 'catch': 218, 'action': 26, 'requir': 1109, 'villag': 1435, 'project': 1041, 'band': 121, 'color': 269, 'teenag': 1345, 'disappear': 388, 'vex': 1432, 'reasons': 1080, 'rang': 1069, 'mate': 842, 'quiet': 1062, 'suit': 1318, 'key': 732, 'assist': 99, 'anatomi': 59, 'compar': 277, 'evening': 456, 'rude': 1140, 'circumst': 247, 'happy': 630, 'relief': 1101, 'departur': 366, 'fierc': 524, 'resign': 1112, 'occas': 922, 'grow': 615, 'statement': 1278, 'spoke': 1264, 'televis': 1348, 'drown': 416, 'contest': 295, 'clock': 256, 'counter': 310, 'lectures': 769, 'biolog': 155, 'rumour': 1142, 'headmast': 637, 'negat': 902, 'taxi': 1338, 'zambia': 1499, 'henri': 650, 'intend': 710, 'hall': 623, 'anxieti': 71, 'sadness': 1147, 'sight': 1204, 'ski': 1220, 'promot': 1043, 'glass': 589, 'despair': 375, 'lover': 812, 'bottl': 178, 'common': 274, 'contact': 293, 'deni': 363, 'glum': 592, 'process': 1035, 'faith': 501, 'mountain': 886, 'headach': 636, 'protect': 1047, 'doubt': 405, 'princ': 1028, 'soldier': 1237, 'conflict': 287, 'essay': 454, 'hostel': 673, 'confess': 285, 'cough': 307, 'admir': 30, 'lee': 770, 'glee': 590, 'theori': 1359, 'sort': 1249, 'lifelong': 782, 'wake': 1440, 'gas': 575, 'warm': 1445, 'space': 1254, 'awkward': 116, 'brought': 193, 'rose': 1135, 'alcohol': 49, 'definit': 356, 'sheer': 1190, 'task': 1336, 'neglect': 903, 'spider': 1262, 'subsequ': 1310, 'soul': 1250, 'assum': 100, 'prove': 1048, 'fals': 503, 'deceiv': 349, 'tremend': 1399, 'relax': 1099, 'light': 784, 'cash': 216, 'intens': 711, 'smart': 1229, 'disgust': 393, 'council': 308, 'narrow': 894, 'rate': 1070, 'lonely': 800, 'pleasant': 997, 'rise': 1125, 'deserv': 370, 'terror': 1355, 'insist': 706, 'dan': 333, 'exist': 480, 'guilti': 617, '11': 2, 'skate': 1219, 'medicine': 854, 'occur': 923, '40': 11, 'affect': 36, 'ann': 66, 'smith': 1232, 'celebr': 221, 'avail': 111, 'truck': 1404, 'feelings': 518, 'body': 171, 'conveni': 298, 'fri': 557, 'nah': 893}\n"
     ]
    }
   ],
   "source": [
    "#print out words in vocabulary (aka dictionary)\n",
    "print(vectorizer.vocabulary_)#much larger vocabulary because the features were increased within the above block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #original KNN method for training: Gave ~39 precent accuracy w no preprocessing. \n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors = 3, metric = 'euclidean')\n",
    "# knn.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the Support Vector Machines classifier from sklearn. \n",
    "#use the Radial Basis Function for the kernel because this will provide best results becaice we habe 5 emotions to\n",
    "#train for this prediction\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "clf = svm.SVC(kernel = 'rbf')\n",
    "#higher number of features can cause a large chance for the dataset to be overfit. \n",
    "clf.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = knn.predict(test_vectors)\n",
    "prediction = clf.predict(test_vectors) #get new preciction based on the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.60      0.68      0.64       458\n",
      "        fear       0.74      0.67      0.70       447\n",
      "         joy       0.67      0.67      0.67       460\n",
      "     neutral       0.70      0.67      0.68       451\n",
      "     sadness       0.68      0.68      0.68       450\n",
      "\n",
      "    accuracy                           0.67      2266\n",
      "   macro avg       0.68      0.67      0.68      2266\n",
      "weighted avg       0.68      0.67      0.68      2266\n",
      "\n",
      "Accuracy: 0.6743159752868491\n"
     ]
    }
   ],
   "source": [
    "#import and print cassificaion report and accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, prediction))\n",
    "\n",
    "#Model Accuracy: how often is the classifier correct\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
